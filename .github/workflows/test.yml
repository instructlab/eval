# SPDX-License-Identifier: Apache-2.0

name: Test

on:
  push:
    branches:
      - "main"
      - "release-**"
    paths:
      - '**.py'
      - 'pyproject.toml'
      - 'requirements*.txt'
      - '.github/workflows/test.yml'
  pull_request:
    branches:
      - "main"
      - "release-**"
    paths:
      - '**.py'
      - 'pyproject.toml'
      - 'requirements*.txt'
      - '.github/workflows/test.yml'

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  unit:
    runs-on: ubuntu-gpu
    steps:
      # No step-security/harden-runner since this is a self-hosted runner
      - name: Checkout instructlab/eval
        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332 # v4.1.7
        with:
          # https://github.com/actions/checkout/issues/249
          fetch-depth: 0

      # this is needed for branch tests
      - name: Checkout instructlab/taxonomy
        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332 # v4.1.7
        with:
          repository: "instructlab/taxonomy"
          path: "taxonomy"
          fetch-depth: 0

      # this is needed for judge_answer tests
      - name: Checkout instructlab/instructlab
        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332 # v4.1.7
        with:
          repository: "instructlab/instructlab"
          path: "instructlab"
          fetch-depth: 0

      - name: Install system packages
        run: |
          sudo apt-get install -y cuda-toolkit git cmake build-essential virtualenv
          nvidia-smi
          sudo ls -l /dev/nvidia*

      - name: Setup Python 3.11
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d # v5.1.0
        with:
          python-version: 3.11
          cache: pip
          cache-dependency-path: |
            **/pyproject.toml
            **/requirements*.txt

      - name: Remove llama-cpp-python from cache
        run: |
          pip cache remove llama_cpp_python

      - name: Start inference server
        run: |
          export PATH="/home/runner/.local/bin:/usr/local/cuda/bin:$PATH"
          cd instructlab
          python3.11 -m venv cli_venv
          . cli_venv/bin/activate
          sed 's/\[.*\]//' requirements.txt > constraints.txt
          python3.11 -m pip cache remove llama_cpp_python
          CMAKE_ARGS="-DLLAMA_CUBLAS=on" python3 -m pip install --no-binary llama_cpp_python -c constraints.txt llama_cpp_python
          # needed for --4-bit-quant option to ilab train
          python3.11 -m pip install bitsandbytes
          # install instructlab
          python3.11 -m pip install .
          # start llama-cpp server
          ilab model download --repository instructlab/granite-7b-lab-GGUF --filename granite-7b-lab-Q4_K_M.gguf
          ilab model serve --model-path /home/runner/.local/share/instructlab/models/granite-7b-lab-Q4_K_M.gguf

      - name: Install dependencies
        run: |
          python3.11 -m venv venv
          . venv/bin/activate
          python3.11 -m pip install .
          python3.11 -m pip install pytest

      - name: Run unit tests
        run: |
          export INSTRUCTLAB_EVAL_FIRST_N_QUESTIONS=5
          . venv/bin/activate
          python3.11 -m pytest

      - name: Remove llama-cpp-python from cache
        if: always()
        run: |
          pip cache remove llama_cpp_python
